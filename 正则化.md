# 正则化

## 欠拟合，正确拟合，过拟合

下图为三种拟合的可视化
函数分别为
$y=\theta_0+\theta_1x\newline y=\theta_0+\theta_1x+\theta_2x^2\newline y=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$
![拟合](https://i.loli.net/2019/12/28/G48HkRPsuacn3hT.png)

- 欠拟合未能很好的找到数据的特征
- 过拟合使模型不能很好的推广

## 代价函数

为了减小$\theta_3,\theta_4$的影响，需要改变代价函数
**eg：**
$\displaystyle{min_\theta \frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+1000\theta_3+1000\theta_4}$
注：1000只是示例，代表一个很大的数。这样的处理后，想要最小化代价函数就必须使$\theta_3,\theta_4\approx0$
使我们的假设得到的结果更趋近于例子中的二次函数
![代价函数](https://i.loli.net/2019/12/28/wfx7siYjc8qMUBl.png)
同时处理所有系数的公式如下
$\displaystyle{J(\theta)= \frac{1}{2m}\bigg[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2\bigg]}$
> *tips:*
> *没有$\theta_0$*
> *$\lambda$叫做正则化系数(regularization parameter)决定了我们惩罚系数的程度*

### logistics回归

代价函数$J(\theta)=\displaystyle{-\frac{1}{m} \sum^m_{i=1} \bigg[y^{(i)}log(h_\theta(x^{(i)}) )-(1-y^{(i)})log(1-h_\theta(x^{(i)}))  \bigg] }+\frac{\lambda}{2m} \sum^n_{j=1}\theta_j^2$

## 梯度下降


$ \text{Repeat}\ \lbrace \newline  \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline  \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] \ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline  \rbrace $

## 正规方程

$ \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline \text{where}\ \ L = \begin{bmatrix} 0  \\&1\\&& 1\\&&&\ddots \\&&&&1\end{bmatrix}$
可以证明$\left(X^TX + \lambda \cdot L \right)^{-1}$一定不是奇异阵