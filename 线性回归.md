# 一元线性回归

模型 $h(x)=\theta_0+\theta_1x$(hypothesis)
目标：构建最合适的模型解释$(x_i,y_i)$数据对即直线预测的值离所有真实的点尽可能近。
预测值 $ \widehat y=h(x)$ 真实值 $y_i$
代价函数（cost function）： $J(\theta_0,\theta_1)=\frac{1}{2m}{{m\atop\sum}\atop i}{(\underbrace{h_\theta(x_{i})}_{h(x)=\theta_0+\theta_1x}-y_{i})^2}$
优化目标： min$\big(J(\theta_0,\theta_1)\big)$
理想的情况下直线可以过所有点，此时$J(\theta_0,\theta_1)=0$。不过大多数情况下不存在$J(\theta_0,\theta_1)$为0的直线。
方法：**梯度下降法**(gradient descent)。

梯度下降法的算法：

1. 初始化$\theta_0,\theta_1$
2. 重复直到收敛:{
    $\theta_j:=\theta_j-{\alpha}\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$(for j=1 and j=0)
}

>tips：

>1. ":="赋值运算符
>2. $\alpha$表示学习率

>>+ $\alpha$太大可能会使函数无法收敛
>>+ $\alpha$太小会使迭代次数过多
>>+ 由于每次迭代更加接近局部最小点，会使步长在$\alpha$不变的情况下越来越小

>3. 需要同时更新$\theta_0,\theta_1$
>4. 一元线性回归的代价函数**始终**为凸函数，不存在局部的最小值
>5. 正规方程法也可得到最小值，但梯度下降法更适合在大数据条件下运行

以上的梯度下降法被称为**批次梯度下降法(Batch Gradient Descent)**，
因为每次计算梯度都是用所有样本$\sum^m_i$
