## 一元线性回归

模型 $h(x)=\theta_0+\theta_1x$(hypothesis假设)
目标：构建最合适的模型解释$(x_i,y_i)$数据对即直线预测的值离所有真实的点尽可能近。
预测值 $ \widehat y=h(x)$ 真实值 $y_i$
代价函数（cost function）： $J(\theta_0,\theta_1)=\frac{1}{2m}{\displaystyle\sum^m_i}{(\underbrace{h_\theta(x_{i})}_{h(x)=\theta_0+\theta_1x}-y_{i})^2}$
优化目标： min$\big(J(\theta_0,\theta_1)\big)$
理想的情况下直线可以过所有点，此时$J(\theta_0,\theta_1)=0$。不过大多数情况下不存在$J(\theta_0,\theta_1)$为0的直线。
方法：**梯度下降法**(gradient descent)。

梯度下降法的算法：

1. 初始化$\theta_0,\theta_1$
2. 重复直到收敛:{
    $\theta_j:=\theta_j-{\alpha}\frac{\displaystyle\partial}{\displaystyle\partial\theta_j}J(\theta_0,\theta_1)=\theta_j-\alpha{\displaystyle\frac1{m}}\displaystyle\sum^m_i(h_\theta(x_i)-y_i)x_j$(for j=1 and j=0)
}

>tips：

>1. ":="赋值运算符
>2. $\alpha$表示学习率

>>+ $\alpha$太大可能会使函数无法收敛
>>+ $\alpha$太小会使迭代次数过多
>>+ 由于每次迭代更加接近局部最小点，会使步长在$\alpha$不变的情况下越来越小

>3. 需要同时更新$\theta_0,\theta_1$
>4. 一元线性回归的代价函数**始终**为凸函数，不存在局部的最小值
>5. 正规方程法也可得到最小值，但梯度下降法更适合在大数据条件下运行

以上的梯度下降法被称为**批次梯度下降法(Batch Gradient Descent)**，
因为每次计算梯度都是用所有样本$\displaystyle\sum^m_i$

## 多元线性回归

$\quad x_0\qquad x_1\qquad x_2\quad\cdots\quad x_n\qquad y$
$\begin{bmatrix}
{x_0^{(1)}}&x_1^{(1)}&x_2^{(1)}&\cdots&x_n^{(1)}\\
{x_0^{(2)}}&x_1^{(2)}&x_2^{(2)}&\cdots&x_n^{(2)}\\
{x_0^{(3)}}&x_1^{(3)}&x_2^{(3)}&\cdots&x_n^{(3)}\\
{\vdots}&{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{x_0^{(m)}}&x_1^{(m)}&x_2^{(m)}&\cdots&x_n^{(m)}\\
\end{bmatrix}$
>符号：

>+ n表示变量的数量,m表示样本的数量
>+ $x_j$表示第j个变量
>+ $x^{(i)}$表示所有变量里的第i个训练样本,是一个向量。$x^{(i)}=\begin{bmatrix}{x_0^{(i)}}\\{x_1^{(i)}}\\{x_2^{(i)}}\\{\vdots}\\{x_n^{(i)}}\\\end{bmatrix}$
>+ $x^{(i)}_j$表示第j个变量里的第i个训练样本

hypothesis：$h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2\cdots\theta_nx_n$(always $x_0$=1)

$x=\begin{bmatrix}
{x_0}\\{x_1}\\{x_2}\\{\vdots}\\{x_n}
\end{bmatrix}$  $\theta=\begin{bmatrix}
{\theta_0}\\{\theta_1}\\{\theta_2}\\{\vdots}\\{\theta_n}
\end{bmatrix}$

hypothesis : $h_\theta(x)=\begin{bmatrix}
{\theta_0}&{\theta_1}&{\theta_2}&{\cdots}&{\theta_n}
\end{bmatrix}\begin{bmatrix}
{x_0}\\{x_1}\\{x_2}\\{\vdots}\\{x_n}
\end{bmatrix}=\theta^Tx$

多元线性回归的梯度下降法：

1. 初始化$\theta_j$

2. 重复直到收敛:{
    $\theta_j:=\theta_j-{\alpha}\frac{\displaystyle\partial}{\displaystyle\partial\theta_j}J(\theta)=\theta_j-\alpha{\displaystyle\frac1{m}}\displaystyle\sum^m_i\big((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\big)$(for j=1 and j=0)
}

### 特征缩放（feature scaling）即归一化

作用：使梯度下降法更快的收敛
目标：将特征取值映射到[-1,1]的区间里
方法：$x_i:=\displaystyle\frac{x_i-\mu_i}{s_i}$($\mu_i$为均值，$s_i$为标准差或范围均可)

![正确参数下的梯度下降法](https://i.imgur.com/LrHrVNQ.png)

## 正规方程（the normal equation）（一步到位，不用迭代）

$J(\theta)=a\theta^2+b\theta+c\quad (\theta\in R^{(n+1)})$
$X=\begin{bmatrix}
{(x^{(0)})^T}\\{(x^{(1)})^T}\\{(x^{(2)})^T}\\{\vdots}\\{(x^{(m)})^T}
\end{bmatrix}$
y即样本真实的取值。
公式：$\theta=(X^TX)^{-1}X^Ty$
>tips:
>+ 不需要将特征向量归一化(一步到位)
>+ 时间复杂度接近$O(n^3)$ <u>n在$10^6$以下推荐使用</u>
>+ octave代码:pinv(x'*x)*x'*y
>+ inv()与pinv()存在区别

![正规方程法](https://i.imgur.com/KFyujsY.png)


### 正规方程的不可逆，即$X^TX$不可逆（e.g. m<=n）

解决方法

1. 删除一些特征（例如存在共线性）

2. *正则化*
