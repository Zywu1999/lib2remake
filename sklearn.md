# 监督学习（数据集有明确的标签）

## 回归——连续型

### logistic 回归

logistic回归用于==分类==，主要思想——根据现有数据对*分类边界线Decision Boundary*建立回归公式。

logistic回归的函数（最常用的就是二分类，也可用于多分类）是一个<u>二值型</u>的输出函数，输出0或1。
*Heaviside step function*或者直接称为单位阶跃函数具有这种从0到1阶跃的特性，但因为其不易被计算机处理，选择Sigmoid函数。
Sigmoid函数公式如下：$ \sigma(x)=\frac{1}{(1+e^{-z})}$

![Sigmoid函数图像](https://camo.githubusercontent.com/af5d72cf92ba8df4ae969c8e8227265ebc214711/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6d6c2f352e4c6f6769737469632f4c525f332e706e67)

为了实现 Logistic 回归分类器，我们可以在==每个特征上都乘以一个回归系数==（如下公式所示），然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以，Logistic 回归也是一种概率估计，比如这里Sigmoid 函数得出的值为0.5，可以理解为给定数据和参数，数据被分入 1 类的概率为0.5。
$z=w_0x_0+w_1x_1+w_2x_2+...+w_nx_n$
采用向量的写法：$z=w^Tx$。
向量x为**输入的数据**，向量$w$为**需要**优化的参数。优化目标为**准确率**提升。

#### 线性回归与logistic回归的区别

1. Logistic回归在线性回归的实数输出范围加上sigmoid函数，将输出值收敛在0~1之间。其目标函数也因此从差平方和函数变为对数损失函数。
2. 逻辑回归和线性回归都是广义的线性回归，线性回归是使用最小二乘法优化目标函数，而逻辑回归是使用梯度下降或者拟牛顿法。
3. 线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围需要在[0,1]。逻辑回归是一种减少预测范围，将预测值限定为[0,1]间的一种回归模型。因而对于二分类问题，逻辑回归的鲁棒性更好。
4. 逻辑回归是以线性回归为理论支持的，但线性回归模型无法做到sigmoid的非线性形式。Sigmoid可以轻松处理0/1分类问题。

## 分类——离散

# 无监督学习（数据集无标签）